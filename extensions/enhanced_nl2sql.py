import re
import json
import requests
from typing import Dict, List, Optional
from catalog.catalog_manager import CatalogManager


class EnhancedNL2SQL:
    """å¢å¼ºçš„è‡ªç„¶è¯­è¨€è½¬SQL"""

    def __init__(self, catalog: CatalogManager, api_key: str = None, api_type: str = "deepseek"):
        self.catalog = catalog
        self.api_key = api_key
        self.api_type = api_type
        self.query_patterns = self._load_query_patterns()
        self.entity_mapping = self._build_entity_mapping()

        # DeepSeek API é…ç½®
        if api_type == "deepseek":
            self.api_url = "https://api.deepseek.com/chat/completions"
            self.model_name = "deepseek-chat"
        elif api_type == "openai":
            self.api_url = "https://api.openai.com/v1/chat/completions"
            self.model_name = "gpt-3.5-turbo"

    def translate(self, natural_query: str) -> Dict:
        """å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºSQL"""
        print(f"ğŸš€ å¼€å§‹ç¿»è¯‘æŸ¥è¯¢: '{natural_query}'")

        try:
            # 1. é¢„å¤„ç†å’Œå®ä½“è¯†åˆ«
            processed_query = self._preprocess_query(natural_query)
            print(f"ğŸ“ é¢„å¤„ç†å: '{processed_query}'")

            entities = self._extract_entities(processed_query)
            print(f"ğŸ¯ æå–åˆ°çš„å®ä½“: {json.dumps(entities, indent=2, ensure_ascii=False)}")

            # 2. æ¨¡å¼åŒ¹é…ï¼ˆå¿«é€Ÿè·¯å¾„ï¼‰
            pattern_result = self._pattern_matching(processed_query, entities)
            print(f"ğŸ” æ¨¡å¼åŒ¹é…ç»“æœ - ç½®ä¿¡åº¦: {pattern_result.get('confidence', 0):.2f}")

            if pattern_result['confidence'] > 0.8:
                print("âœ… ä½¿ç”¨æ¨¡å¼åŒ¹é…ç»“æœ")
                return self._enhance_result(pattern_result, natural_query)

            # 3. ä½¿ç”¨ DeepSeek API å¤„ç†å¤æ‚æŸ¥è¯¢
            if self.api_key:
                print("ğŸ¤– ä½¿ç”¨AI APIå¤„ç†...")
                ai_result = self._translate_with_deepseek(natural_query, entities)
                print(f"ğŸ­ AIç»“æœç½®ä¿¡åº¦: {ai_result.get('confidence', 0):.2f}")
                return self._enhance_result(ai_result, natural_query)
            else:
                print("âš ï¸ æ— APIå¯†é’¥ï¼Œä½¿ç”¨æ¨¡å¼åŒ¹é…ç»“æœ")
                return self._enhance_result(pattern_result, natural_query)

        except Exception as e:
            print(f"âŒ ç¿»è¯‘è¿‡ç¨‹å‡ºé”™: {str(e)}")
            return {
                'sql': '',
                'confidence': 0.0,
                'explanation': f'è½¬æ¢å¤±è´¥: {str(e)}',
                'suggestions': ['è¯·æ£€æŸ¥æŸ¥è¯¢è¯­å¥çš„è¡¨è¾¾æ–¹å¼'],
                'entities': {},
                'error': str(e),
                'method': 'error'
            }

    def _translate_with_deepseek(self, natural_query: str, entities: Dict) -> Dict:
        """ä½¿ç”¨ DeepSeek API ç¿»è¯‘"""
        try:
            schema_context = self._get_schema_context()

            # æ„å»ºæ›´è¯¦ç»†çš„æç¤ºè¯
            prompt = self._build_detailed_prompt(natural_query, entities, schema_context)

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }

            data = {
                "model": self.model_name,
                "messages": [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„SQLä¸“å®¶ï¼Œä¸“é—¨å°†ä¸­æ–‡è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºå‡†ç¡®çš„SQLè¯­å¥ã€‚ä½ å¿…é¡»æ ¹æ®æä¾›çš„æ•°æ®åº“ç»“æ„ç”Ÿæˆè¯­æ³•æ­£ç¡®çš„SQLã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": 0.1,
                "max_tokens": 1000,
                "stream": False
            }

            response = requests.post(self.api_url, headers=headers, json=data, timeout=30)

            if response.status_code == 200:
                result_data = response.json()
                result_text = result_data['choices'][0]['message']['content']

                # è§£æç»“æœ
                parsed_result = self._parse_api_response(result_text)
                parsed_result['method'] = 'deepseek_api'
                return parsed_result

            else:
                return {
                    'sql': '',
                    'confidence': 0.0,
                    'explanation': f'APIè¯·æ±‚å¤±è´¥: {response.status_code}',
                    'error': f'HTTP {response.status_code}: {response.text}',
                    'method': 'deepseek_api'
                }

        except requests.exceptions.Timeout:
            return {
                'sql': '',
                'confidence': 0.0,
                'explanation': 'APIè¯·æ±‚è¶…æ—¶',
                'error': 'Request timeout',
                'method': 'deepseek_api'
            }
        except Exception as e:
            return {
                'sql': '',
                'confidence': 0.0,
                'explanation': f'DeepSeek APIå¤„ç†å¤±è´¥: {str(e)}',
                'error': str(e),
                'method': 'deepseek_api'
            }

    def _build_detailed_prompt(self, natural_query: str, entities: Dict, schema_context: Dict) -> str:
        """æ„å»ºè¯¦ç»†çš„æç¤ºè¯"""
        prompt = f"""
è¯·å°†ä»¥ä¸‹ä¸­æ–‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºSQLè¯­å¥ã€‚

æ•°æ®åº“ç»“æ„ä¿¡æ¯:
{json.dumps(schema_context, indent=2, ensure_ascii=False)}

å·²è¯†åˆ«çš„å®ä½“ä¿¡æ¯:
- è¡¨å: {entities.get('tables', [])}
- åˆ—å: {[f"{e.get('table', 'unknown')}.{e.get('column', 'unknown')}" for e in entities.get('columns', [])]}
- èšåˆå‡½æ•°: {entities.get('aggregates', [])}
- æ•°å€¼æ¡ä»¶: {entities.get('values', [])}

è‡ªç„¶è¯­è¨€æŸ¥è¯¢: "{natural_query}"

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼ˆç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONï¼‰:
{{
    "sql": "ç”Ÿæˆçš„SQLè¯­å¥ï¼ˆå¿…é¡»ä»¥åˆ†å·ç»“å°¾ï¼‰",
    "explanation": "è¯¦ç»†çš„ä¸­æ–‡è§£é‡Š",
    "confidence": 0.9,
    "reasoning": "æ¨ç†è¿‡ç¨‹è¯´æ˜",
    "tables_used": ["ä½¿ç”¨çš„è¡¨ååˆ—è¡¨"],
    "potential_issues": ["å¯èƒ½çš„é—®é¢˜æˆ–æ³¨æ„äº‹é¡¹"]
}}

é‡è¦è¦æ±‚:
1. SQLè¯­å¥å¿…é¡»è¯­æ³•æ­£ç¡®ä¸”ç¬¦åˆæ ‡å‡†SQLè¯­æ³•
2. åªä½¿ç”¨æä¾›çš„è¡¨åå’Œåˆ—å
3. å¦‚æœæŸ¥è¯¢ä¸æ˜ç¡®ï¼Œé€‰æ‹©æœ€åˆç†çš„è§£é‡Š
4. ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
5. SQLè¯­å¥å¿…é¡»ä»¥åˆ†å·ç»“å°¾
"""
        return prompt

    def _parse_api_response(self, response_text: str) -> Dict:
        """è§£æAPIå“åº”"""
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            if response_text.strip().startswith('{'):
                result = json.loads(response_text)

                # éªŒè¯å¿…è¦å­—æ®µ
                if 'sql' in result:
                    # ç¡®ä¿SQLä»¥åˆ†å·ç»“å°¾
                    sql = result['sql'].strip()
                    if not sql.endswith(';'):
                        sql += ';'
                    result['sql'] = sql

                    # è®¾ç½®é»˜è®¤å€¼
                    result.setdefault('confidence', 0.8)
                    result.setdefault('explanation', 'ç”±AIç”Ÿæˆçš„SQLæŸ¥è¯¢')
                    result.setdefault('reasoning', 'åŸºäºè‡ªç„¶è¯­è¨€ç†è§£ç”Ÿæˆ')

                    return result

            # å¦‚æœä¸æ˜¯JSONï¼Œå°è¯•æå–SQL
            sql_extracted = self._extract_sql_from_text(response_text)
            if sql_extracted:
                return {
                    'sql': sql_extracted,
                    'confidence': 0.7,
                    'explanation': 'ä»AIå“åº”ä¸­æå–çš„SQLè¯­å¥',
                    'reasoning': response_text[:200] + '...' if len(response_text) > 200 else response_text
                }

            # éƒ½å¤±è´¥äº†ï¼Œè¿”å›åŸæ–‡æœ¬ä½œä¸ºè§£é‡Š
            return {
                'sql': '',
                'confidence': 0.0,
                'explanation': f'æ— æ³•è§£æAIå“åº”: {response_text[:200]}...',
                'reasoning': response_text
            }

        except json.JSONDecodeError as e:
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•æå–SQL
            sql_extracted = self._extract_sql_from_text(response_text)
            return {
                'sql': sql_extracted or '',
                'confidence': 0.6 if sql_extracted else 0.0,
                'explanation': f'JSONè§£æå¤±è´¥ï¼Œæå–åˆ°çš„SQL: {sql_extracted or "æ— "}',
                'error': f'JSONè§£æé”™è¯¯: {str(e)}',
                'raw_response': response_text
            }

    def _extract_sql_from_text(self, text: str) -> Optional[str]:
        """ä»æ–‡æœ¬ä¸­æå–SQLè¯­å¥"""
        import re

        # æŸ¥æ‰¾SQLä»£ç å—
        patterns = [
            r'```sql\s*(.*?)\s*```',  # ```sql ä»£ç å—
            r'```\s*(SELECT.*?;)\s*```',  # æ™®é€šä»£ç å—ä¸­çš„SQL
            r'"sql":\s*"([^"]*)"',  # JSONä¸­çš„sqlå­—æ®µ
            r'(SELECT.*?;)',  # ç›´æ¥çš„SQLè¯­å¥
            r'(INSERT.*?;)',
            r'(UPDATE.*?;)',
            r'(DELETE.*?;)',
            r'(CREATE.*?;)',
            r'(DROP.*?;)'
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                sql = match.group(1).strip()
                if not sql.endswith(';'):
                    sql += ';'
                return sql

        return None

    def _preprocess_query(self, query: str) -> str:
        """æŸ¥è¯¢é¢„å¤„ç† - ä¿®å¤ç‰ˆæœ¬"""
        print(f"ğŸ“‹ åŸå§‹æŸ¥è¯¢: '{query}'")

        if not query or not query.strip():
            print("âš ï¸ æŸ¥è¯¢ä¸ºç©º")
            return ""

        # ä¿ç•™åŸå§‹æŸ¥è¯¢ç”¨äºè¡¨ååŒ¹é…
        original_query = query.strip()
        query = query.strip().lower()

        # æ ‡å‡†åŒ–å¸¸è§è¡¨è¾¾ï¼Œä½†ä¿ç•™è¡¨å
        replacements = {
            r'æ˜¾ç¤º|å±•ç¤º|æŸ¥çœ‹|åˆ—å‡º|ç»™æˆ‘çœ‹|æŸ¥è¯¢': 'show',
            r'æ‰€æœ‰çš„?æ•°æ®|å…¨éƒ¨æ•°æ®|å…¨ä½“æ•°æ®': 'all data',  # ä¿®æ”¹è¿™é‡Œ
            r'å‘˜å·¥|é›‡å‘˜|èŒå‘˜': 'employee',
            r'éƒ¨é—¨|ç§‘å®¤': 'department',
            r'è–ªèµ„|å·¥èµ„|è–ªæ°´|æ”¶å…¥': 'salary',
            r'å§“å|åå­—|åç§°': 'name',
            # ç§»é™¤å¯¹"è¡¨"çš„æ›¿æ¢ï¼Œä¿ç•™åŸå§‹è¡¨å
        }

        processed_query = query
        for pattern, replacement in replacements.items():
            processed_query = re.sub(pattern, replacement, processed_query)

        print(f"ğŸ”„ é¢„å¤„ç†ç»“æœ: '{processed_query}'")
        return processed_query
    def _extract_entities(self, query: str) -> Dict:
        """å®ä½“æå–"""
        print(f"ğŸ” å¼€å§‹å®ä½“æå–: '{query}'")

        entities = {
            'tables': [],
            'columns': [],
            'conditions': [],
            'aggregates': [],
            'values': []
        }

        # è·å–æ‰€æœ‰å¯ç”¨çš„è¡¨
        try:
            all_tables = self.catalog.get_all_tables()
            print(f"ğŸ“Š å¯ç”¨è¡¨: {list(all_tables.keys())}")
        except Exception as e:
            print(f"âš ï¸ è·å–è¡¨ä¿¡æ¯å¤±è´¥: {e}")
            all_tables = {}

        # æå–è¡¨åï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
        table_found = False
        for table_name in all_tables.keys():
            # æ£€æŸ¥å®Œæ•´è¡¨åï¼ˆåŒ…æ‹¬æ•°å­—ï¼‰
            if table_name.lower() in query.lower():  # æ”¹ä¸ºä¸åŒºåˆ†å¤§å°å†™
                entities['tables'].append(table_name)
                table_found = True
                print(f"âœ… æ‰¾åˆ°è¡¨å: {table_name} (å®Œå…¨åŒ¹é…)")
            # æ£€æŸ¥è¡¨åçš„å„ç§å˜å½¢
            elif self._fuzzy_table_match(query, table_name):
                entities['tables'].append(table_name)
                table_found = True
                print(f"âœ… æ‰¾åˆ°è¡¨å: {table_name} (æ¨¡ç³ŠåŒ¹é…)")
            # æ£€æŸ¥å•æ•°å½¢å¼
            elif table_name.lower().rstrip('s') in query:
                entities['tables'].append(table_name)
                table_found = True
                print(f"âœ… æ‰¾åˆ°è¡¨å: {table_name} (å•æ•°åŒ¹é…)")
            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            elif self._fuzzy_table_match(query, table_name):
                entities['tables'].append(table_name)
                table_found = True
                print(f"âœ… æ‰¾åˆ°è¡¨å: {table_name} (æ¨¡ç³ŠåŒ¹é…)")

        if not table_found:
            print("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„è¡¨å")
            # å°è¯•ä»å®ä½“æ˜ å°„ä¸­æŸ¥æ‰¾
            for alias, real_name in self.entity_mapping.get('table_aliases', {}).items():
                if alias in query:
                    entities['tables'].append(real_name)
                    print(f"âœ… é€šè¿‡åˆ«åæ‰¾åˆ°è¡¨: {alias} -> {real_name}")

        # æå–åˆ—å
        for table_name, table_info in all_tables.items():
            if table_name in entities['tables']:  # åªæ£€æŸ¥å·²è¯†åˆ«çš„è¡¨
                columns = []
                if isinstance(table_info, dict):
                    columns = [col.get('name', col) if isinstance(col, dict) else col
                               for col in table_info.get('columns', [])]

                for col in columns:
                    if col.lower() in query:
                        entities['columns'].append({'table': table_name, 'column': col})
                        print(f"âœ… æ‰¾åˆ°åˆ—: {table_name}.{col}")

        # æå–æ•°å€¼
        numbers = re.findall(r'\b\d+(?:\.\d+)?\b', query)
        entities['values'].extend(numbers)
        if numbers:
            print(f"ğŸ”¢ æ‰¾åˆ°æ•°å€¼: {numbers}")

        # æå–èšåˆå‡½æ•°
        agg_keywords = {
            'count': 'COUNT',
            'show all': 'SELECT_ALL',
            'all': 'SELECT_ALL',
            'average': 'AVG',
            'avg': 'AVG',
            'sum': 'SUM',
            'max': 'MAX',
            'min': 'MIN'
        }

        for keyword, func in agg_keywords.items():
            if keyword in query:
                entities['aggregates'].append(func)
                print(f"ğŸ“Š æ‰¾åˆ°èšåˆå‡½æ•°: {keyword} -> {func}")

        print(f"ğŸ¯ æœ€ç»ˆæå–ç»“æœ: {json.dumps(entities, indent=2, ensure_ascii=False)}")
        return entities

    def _pattern_matching(self, query: str, entities: Dict) -> Dict:
        """å¢å¼ºçš„æ¨¡å¼åŒ¹é…"""
        print(f"ğŸ” æ¨¡å¼åŒ¹é…å¼€å§‹")
        print(f"  æŸ¥è¯¢: '{query}'")
        print(f"  æ‰¾åˆ°çš„è¡¨: {entities.get('tables', [])}")

        # æ£€æŸ¥æ˜¯å¦æ˜¯æŸ¥è¯¢æ‰€æœ‰æ•°æ®çš„æ¨¡å¼
        all_data_patterns = [
            'show all', 'list all', 'select all', 'all data',
            'æ‰€æœ‰æ•°æ®', 'å…¨éƒ¨æ•°æ®', 'å…¨ä½“æ•°æ®'
        ]

        is_select_all = any(pattern in query for pattern in all_data_patterns)
        print(f"  æ˜¯å¦ä¸ºæŸ¥è¯¢å…¨éƒ¨: {is_select_all}")

        if is_select_all and entities['tables']:
            # é€‰æ‹©æœ€åŒ¹é…çš„è¡¨åï¼Œè€Œä¸æ˜¯ç¬¬ä¸€ä¸ª
            table = self._select_best_table(query, entities['tables'])
            print(f"âœ… æ„å»ºæŸ¥è¯¢ - ä½¿ç”¨è¡¨: {table}")

            result = {
                'sql': f"SELECT * FROM {table};",
                'confidence': 0.9,
                'explanation': f'æŸ¥è¯¢{table}è¡¨çš„æ‰€æœ‰è®°å½•',
                'pattern': 'select_all',
                'method': 'pattern_matching'
            }

            print(f"ğŸ“ ç”ŸæˆSQL: {result['sql']}")
            return result

        # æ¡ä»¶æŸ¥è¯¢æ¨¡å¼
        if entities['conditions'] or any(op in query for op in ['greater than', 'less than', 'equals']):
            return self._build_conditional_query(query, entities)

        # èšåˆæŸ¥è¯¢æ¨¡å¼
        if entities['aggregates']:
            return self._build_aggregate_query(query, entities)

        # åˆ†ç»„æŸ¥è¯¢æ¨¡å¼
        if 'group by' in query or 'by department' in query:
            return self._build_group_by_query(query, entities)

        print("âŒ æœªåŒ¹é…åˆ°ä»»ä½•æ¨¡å¼")
        return {'sql': '', 'confidence': 0.0, 'pattern': 'unknown'}

    def _select_best_table(self, query: str, tables: List[str]) -> str:
        """ä»å¤šä¸ªè¡¨ä¸­é€‰æ‹©æœ€åŒ¹é…çš„è¡¨"""
        print(f"ğŸ¯ é€‰æ‹©æœ€ä½³è¡¨å")
        print(f"  å€™é€‰è¡¨: {tables}")
        print(f"  æŸ¥è¯¢: '{query}'")

        if len(tables) == 1:
            print(f"  åªæœ‰ä¸€ä¸ªè¡¨ï¼Œç›´æ¥é€‰æ‹©: {tables[0]}")
            return tables[0]

        # è®¡ç®—æ¯ä¸ªè¡¨åçš„åŒ¹é…åˆ†æ•°
        table_scores = {}

        for table in tables:
            score = 0
            print(f"  è¯„ä¼°è¡¨ '{table}':")

            # 1. å®Œå…¨åŒ¹é…å¾—åˆ†æœ€é«˜
            if table.lower() in query.lower():
                exact_matches = len([m for m in re.finditer(re.escape(table.lower()), query.lower())])
                score += exact_matches * 10
                print(f"    å®Œå…¨åŒ¹é…æ¬¡æ•°: {exact_matches}, å¾—åˆ†: +{exact_matches * 10}")

            # 2. é•¿åº¦ä¼˜å…ˆï¼ˆæ›´å…·ä½“çš„è¡¨åï¼‰
            score += len(table) * 0.1
            print(f"    é•¿åº¦å¾—åˆ†: +{len(table) * 0.1}")

            # 3. åŒ…å«æ•°å­—çš„è¡¨åä¼˜å…ˆï¼ˆå¦‚æœæŸ¥è¯¢ä¸­æåˆ°äº†æ•°å­—ï¼‰
            if re.search(r'\d', table) and re.search(r'\d', query):
                score += 5
                print(f"    æ•°å­—åŒ¹é…å¾—åˆ†: +5")

            # 4. æ£€æŸ¥è¡¨ååœ¨æŸ¥è¯¢ä¸­çš„ä½ç½®ï¼ˆè¶Šé å‰è¶Šé‡è¦ï¼‰
            try:
                position = query.lower().find(table.lower())
                if position >= 0:
                    # ä½ç½®è¶Šé å‰å¾—åˆ†è¶Šé«˜
                    position_score = max(0, 10 - position * 0.1)
                    score += position_score
                    print(f"    ä½ç½®å¾—åˆ†: +{position_score}")
            except:
                pass

            table_scores[table] = score
            print(f"    æ€»å¾—åˆ†: {score}")

        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„è¡¨
        best_table = max(table_scores, key=table_scores.get)
        print(f"ğŸ† æœ€ä½³åŒ¹é…: {best_table} (å¾—åˆ†: {table_scores[best_table]})")

        return best_table

    def _fuzzy_table_match(self, query: str, table_name: str) -> bool:
        """æ¨¡ç³Šè¡¨ååŒ¹é…"""
        # æ£€æŸ¥ordersè¡¨çš„ç‰¹æ®Šæƒ…å†µ
        if 'order' in table_name.lower() and ('order' in query or 'orders' in query):
            return True

        # æ£€æŸ¥employeeç›¸å…³
        if 'employee' in table_name.lower() and 'employee' in query:
            return True

        # æ£€æŸ¥departmentç›¸å…³
        if 'department' in table_name.lower() and 'department' in query:
            return True

        return False

    def _build_conditional_query(self, query: str, entities: Dict) -> Dict:
        """æ„å»ºæ¡ä»¶æŸ¥è¯¢"""
        if not entities['tables']:
            return {'sql': '', 'confidence': 0.0}

        # ä½¿ç”¨æœ€ä½³åŒ¹é…çš„è¡¨å
        table = self._select_best_table(query, entities['tables'])
        sql_parts = [f"SELECT * FROM {table}"]

        conditions = []

        # è§£ææ•°å€¼æ¡ä»¶
        for value in entities['values']:
            if 'greater than' in query:
                # çŒœæµ‹æ˜¯salaryæ¡ä»¶
                if any(col['column'] == 'salary' for col in entities['columns']):
                    conditions.append(f"salary > {value}")
                else:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°å€¼åˆ—
                    numeric_cols = self._get_numeric_columns(table)
                    if numeric_cols:
                        conditions.append(f"{numeric_cols[0]} > {value}")

            elif 'less than' in query:
                numeric_cols = self._get_numeric_columns(table)
                if numeric_cols:
                    conditions.append(f"{numeric_cols[0]} < {value}")

        if conditions:
            sql_parts.append(f"WHERE {' AND '.join(conditions)}")

        sql = ' '.join(sql_parts) + ';'

        return {
            'sql': sql,
            'confidence': 0.8,
            'explanation': f'ä»{table}è¡¨ä¸­æŸ¥è¯¢æ»¡è¶³æ¡ä»¶çš„è®°å½•',
            'pattern': 'conditional'
        }

    def _build_aggregate_query(self, query: str, entities: Dict) -> Dict:
        """æ„å»ºèšåˆæŸ¥è¯¢"""
        print(f"ğŸ“Š æ„å»ºèšåˆæŸ¥è¯¢")

        if not entities['tables']:
            print("âŒ æ²¡æœ‰è¡¨åï¼Œæ— æ³•æ„å»ºèšåˆæŸ¥è¯¢")
            return {'sql': '', 'confidence': 0.0}

        # ä½¿ç”¨æœ€ä½³åŒ¹é…çš„è¡¨å
        table = self._select_best_table(query, entities['tables'])

        # å¤„ç†SELECT_ALLç±»å‹
        if 'SELECT_ALL' in entities['aggregates']:
            sql = f"SELECT * FROM {table};"
            explanation = f"æŸ¥è¯¢{table}è¡¨çš„æ‰€æœ‰è®°å½•"
            confidence = 0.9
        elif 'COUNT' in entities['aggregates']:
            sql = f"SELECT COUNT(*) as count FROM {table};"
            explanation = f"ç»Ÿè®¡{table}è¡¨çš„è®°å½•æ•°"
            confidence = 0.85
        else:
            # å…¶ä»–èšåˆå‡½æ•°
            aggregate = entities['aggregates'][0]
            numeric_cols = self._get_numeric_columns(table)

            if numeric_cols:
                target_column = numeric_cols[0]
                sql = f"SELECT {aggregate}({target_column}) as result FROM {table};"
                explanation = f"è®¡ç®—{table}è¡¨ä¸­{target_column}åˆ—çš„{aggregate.lower()}"
                confidence = 0.8
            else:
                sql = f"SELECT COUNT(*) as count FROM {table};"
                explanation = f"ç»Ÿè®¡{table}è¡¨çš„è®°å½•æ•°ï¼ˆæœªæ‰¾åˆ°æ•°å€¼åˆ—ï¼‰"
                confidence = 0.6

        result = {
            'sql': sql,
            'confidence': confidence,
            'explanation': explanation,
            'pattern': 'aggregate',
            'method': 'pattern_matching'
        }

        print(f"ğŸ“Š èšåˆæŸ¥è¯¢ç»“æœ: {result}")
        return result

    def _build_group_by_query(self, query: str, entities: Dict) -> Dict:
        """æ„å»ºåˆ†ç»„æŸ¥è¯¢"""
        if not entities['tables']:
            return {'sql': '', 'confidence': 0.0}

        # ä½¿ç”¨æœ€ä½³åŒ¹é…çš„è¡¨å
        table = self._select_best_table(query, entities['tables'])

        # é»˜è®¤æŒ‰éƒ¨é—¨åˆ†ç»„
        group_by_col = 'department'

        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–åˆ†ç»„åˆ—
        for col_info in entities.get('columns', []):
            col_name = col_info.get('column', '')
            if 'department' in col_name.lower() or 'dept' in col_name.lower():
                group_by_col = col_name
                break

        # ç¡®å®šèšåˆå‡½æ•°
        if entities.get('aggregates'):
            agg_func = entities['aggregates'][0]
            if agg_func == 'COUNT':
                sql = f"SELECT {group_by_col}, COUNT(*) as count FROM {table} GROUP BY {group_by_col};"
                explanation = f"æŒ‰{group_by_col}åˆ†ç»„ç»Ÿè®¡{table}è¡¨çš„è®°å½•æ•°"
            else:
                # æ‰¾åˆ°åˆé€‚çš„èšåˆåˆ—
                agg_col = self._get_numeric_columns(table)
                if agg_col:
                    sql = f"SELECT {group_by_col}, {agg_func}({agg_col[0]}) as result FROM {table} GROUP BY {group_by_col};"
                    explanation = f"æŒ‰{group_by_col}åˆ†ç»„è®¡ç®—{table}è¡¨ä¸­{agg_col[0]}çš„{agg_func.lower()}"
                else:
                    sql = f"SELECT {group_by_col}, COUNT(*) as count FROM {table} GROUP BY {group_by_col};"
                    explanation = f"æŒ‰{group_by_col}åˆ†ç»„ç»Ÿè®¡{table}è¡¨çš„è®°å½•æ•°"
        else:
            # é»˜è®¤ä½¿ç”¨COUNT
            sql = f"SELECT {group_by_col}, COUNT(*) as count FROM {table} GROUP BY {group_by_col};"
            explanation = f"æŒ‰{group_by_col}åˆ†ç»„ç»Ÿè®¡{table}è¡¨çš„è®°å½•æ•°"

        return {
            'sql': sql,
            'confidence': 0.8,
            'explanation': explanation,
            'pattern': 'group_by'
        }

    def _ai_enhanced_translation(self, natural_query: str, entities: Dict) -> Dict:
        """AIå¢å¼ºçš„ç¿»è¯‘"""
        schema_context = self._get_schema_context()

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„SQLç”ŸæˆåŠ©æ‰‹ã€‚æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºSQLè¯­å¥ã€‚

æ•°æ®åº“ç»“æ„:
{json.dumps(schema_context, indent=2, ensure_ascii=False)}

å·²è¯†åˆ«çš„å®ä½“:
- è¡¨: {entities.get('tables', [])}
- åˆ—: {[f"{e['table']}.{e['column']}" for e in entities.get('columns', [])]}
- èšåˆå‡½æ•°: {entities.get('aggregates', [])}
- æ•°å€¼: {entities.get('values', [])}

è‡ªç„¶è¯­è¨€æŸ¥è¯¢: {natural_query}

è¯·ç”ŸæˆSQLè¯­å¥å¹¶æä¾›ä»¥ä¸‹JSONæ ¼å¼çš„å“åº”:
{{
    "sql": "ç”Ÿæˆçš„SQLè¯­å¥",
    "explanation": "ä¸­æ–‡è§£é‡Š",
    "confidence": 0.9,
    "reasoning": "æ¨ç†è¿‡ç¨‹"
}}
"""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ï¼Œä¸“é—¨å°†ä¸­æ–‡è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºå‡†ç¡®çš„SQLè¯­å¥ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1
            )

            result_text = response.choices[0].message.content
            result = json.loads(result_text)
            result['method'] = 'ai_enhanced'

            # éªŒè¯ç”Ÿæˆçš„SQL
            if self._validate_generated_sql(result.get('sql', '')):
                result['confidence'] = min(result.get('confidence', 0.7) + 0.1, 1.0)

            return result

        except Exception as e:
            return {
                'sql': '',
                'confidence': 0.0,
                'explanation': f'AIå¤„ç†å¤±è´¥: {str(e)}',
                'method': 'ai_enhanced',
                'error': str(e)
            }

    def _get_schema_context(self) -> Dict:
        """è·å–æ•°æ®åº“ç»“æ„ä¸Šä¸‹æ–‡"""
        schema = {'tables': {}, 'views': {}}

        # è·å–æ‰€æœ‰è¡¨
        try:
            tables = self.catalog.get_all_tables()
            for table_name, table_info in tables.items():
                columns_info = []
                for col in table_info.get('columns', []):
                    col_info = {
                        'name': col.get('name', col) if isinstance(col, dict) else col,
                        'type': col.get('type', 'VARCHAR') if isinstance(col, dict) else 'VARCHAR'
                    }
                    columns_info.append(col_info)

                schema['tables'][table_name] = {
                    'columns': columns_info,
                    'description': f'è¡¨ {table_name}'
                }
        except Exception as e:
            # å¦‚æœè·å–è¡¨ä¿¡æ¯å¤±è´¥ï¼Œè¿”å›ç©ºç»“æ„
            schema['tables'] = {}

        # è·å–æ‰€æœ‰è§†å›¾
        try:
            if hasattr(self.catalog, 'get_all_views'):
                views = self.catalog.get_all_views()
                for view_name in views:
                    schema['views'][view_name] = {
                        'type': 'view',
                        'description': f'è§†å›¾ {view_name}'
                    }
        except Exception:
            schema['views'] = {}

        return schema

    def _get_numeric_columns(self, table_name: str) -> List[str]:
        """è·å–è¡¨çš„æ•°å€¼ç±»å‹åˆ—"""
        numeric_columns = []
        try:
            all_tables = self.catalog.get_all_tables()
            if table_name in all_tables:
                table_info = all_tables[table_name]
                columns = table_info.get('columns', [])

                for col in columns:
                    col_name = col.get('name', col) if isinstance(col, dict) else col
                    col_type = col.get('type', '') if isinstance(col, dict) else ''

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å€¼ç±»å‹
                    if any(num_type in col_type.upper() for num_type in ['INT', 'FLOAT', 'DECIMAL', 'NUMBER']):
                        numeric_columns.append(col_name)
                    elif col_name.lower() in ['salary', 'age', 'price', 'amount', 'count', 'id']:
                        # åŸºäºåˆ—åæ¨æµ‹
                        numeric_columns.append(col_name)

            print(f"ğŸ”¢ è¡¨ {table_name} çš„æ•°å€¼åˆ—: {numeric_columns}")
        except Exception as e:
            print(f"âš ï¸ è·å–æ•°å€¼åˆ—å¤±è´¥: {e}")

        return numeric_columns

    def _build_group_by_query(self, query: str, entities: Dict) -> Dict:
        """æ„å»ºåˆ†ç»„æŸ¥è¯¢"""
        if not entities['tables']:
            return {'sql': '', 'confidence': 0.0}

        table = entities['tables'][0]

        # é»˜è®¤æŒ‰éƒ¨é—¨åˆ†ç»„
        group_by_col = 'department'

        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–åˆ†ç»„åˆ—
        for col_info in entities.get('columns', []):
            col_name = col_info.get('column', '')
            if 'department' in col_name.lower() or 'dept' in col_name.lower():
                group_by_col = col_name
                break

        # ç¡®å®šèšåˆå‡½æ•°
        if entities.get('aggregates'):
            agg_func = entities['aggregates'][0]
            if agg_func == 'COUNT':
                sql = f"SELECT {group_by_col}, COUNT(*) as count FROM {table} GROUP BY {group_by_col};"
                explanation = f"æŒ‰{group_by_col}åˆ†ç»„ç»Ÿè®¡{table}è¡¨çš„è®°å½•æ•°"
            else:
                # æ‰¾åˆ°åˆé€‚çš„èšåˆåˆ—
                agg_col = self._get_numeric_columns(table)
                if agg_col:
                    sql = f"SELECT {group_by_col}, {agg_func}({agg_col[0]}) as result FROM {table} GROUP BY {group_by_col};"
                    explanation = f"æŒ‰{group_by_col}åˆ†ç»„è®¡ç®—{table}è¡¨ä¸­{agg_col[0]}çš„{agg_func.lower()}"
                else:
                    sql = f"SELECT {group_by_col}, COUNT(*) as count FROM {table} GROUP BY {group_by_col};"
                    explanation = f"æŒ‰{group_by_col}åˆ†ç»„ç»Ÿè®¡{table}è¡¨çš„è®°å½•æ•°"
        else:
            # é»˜è®¤ä½¿ç”¨COUNT
            sql = f"SELECT {group_by_col}, COUNT(*) as count FROM {table} GROUP BY {group_by_col};"
            explanation = f"æŒ‰{group_by_col}åˆ†ç»„ç»Ÿè®¡{table}è¡¨çš„è®°å½•æ•°"

        return {
            'sql': sql,
            'confidence': 0.8,
            'explanation': explanation,
            'pattern': 'group_by'
        }

    def _enhance_result(self, result: Dict, original_query: str) -> Dict:
        """å¢å¼ºç»“æœä¿¡æ¯"""
        enhanced_result = result.copy()

        # æ·»åŠ åŸå§‹æŸ¥è¯¢
        enhanced_result['original_query'] = original_query

        # æ·»åŠ æ—¶é—´æˆ³
        from datetime import datetime
        enhanced_result['timestamp'] = datetime.now().isoformat()

        # å¦‚æœæœ‰SQLï¼Œæ·»åŠ éªŒè¯ä¿¡æ¯
        if enhanced_result.get('sql'):
            enhanced_result['sql_valid'] = self._validate_generated_sql(enhanced_result['sql'])

        # æ·»åŠ å»ºè®®
        if not enhanced_result.get('suggestions'):
            enhanced_result['suggestions'] = self._generate_suggestions(enhanced_result)

        return enhanced_result

    def _validate_generated_sql(self, sql: str) -> bool:
        """éªŒè¯ç”Ÿæˆçš„SQLè¯­å¥"""
        if not sql or not sql.strip():
            return False

        sql = sql.strip().upper()

        # åŸºæœ¬è¯­æ³•æ£€æŸ¥
        valid_starts = ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE', 'DROP', 'ALTER']
        if not any(sql.startswith(start) for start in valid_starts):
            return False

        # æ£€æŸ¥æ˜¯å¦ä»¥åˆ†å·ç»“å°¾
        if not sql.endswith(';'):
            return False

        # æ£€æŸ¥æ‹¬å·åŒ¹é…
        if sql.count('(') != sql.count(')'):
            return False

        # æ£€æŸ¥åŸºæœ¬çš„SELECTè¯­å¥ç»“æ„
        if sql.startswith('SELECT'):
            if 'FROM' not in sql:
                return False

        return True

    def _generate_suggestions(self, result: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []

        confidence = result.get('confidence', 0)

        if confidence < 0.5:
            suggestions.append("æŸ¥è¯¢è½¬æ¢ç½®ä¿¡åº¦è¾ƒä½ï¼Œè¯·æ£€æŸ¥è‡ªç„¶è¯­è¨€è¡¨è¾¾æ˜¯å¦å‡†ç¡®")
            suggestions.append("å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„è¡¨åå’Œåˆ—å")

        if confidence < 0.8:
            suggestions.append("å»ºè®®éªŒè¯ç”Ÿæˆçš„SQLè¯­å¥æ˜¯å¦ç¬¦åˆé¢„æœŸ")

        sql = result.get('sql', '')
        if sql:
            if 'SELECT *' in sql.upper():
                suggestions.append("ä½¿ç”¨SELECT *å¯èƒ½å½±å“æ€§èƒ½ï¼Œå»ºè®®æŒ‡å®šå…·ä½“åˆ—å")

            if 'WHERE' not in sql.upper() and 'SELECT' in sql.upper():
                suggestions.append("è€ƒè™‘æ·»åŠ WHEREæ¡ä»¶ä»¥æé«˜æŸ¥è¯¢æ•ˆç‡")

        if not suggestions:
            suggestions.append("SQLç”ŸæˆæˆåŠŸï¼Œå»ºè®®åœ¨æ‰§è¡Œå‰å…ˆéªŒè¯ç»“æœ")

        return suggestions

    def _load_query_patterns(self) -> Dict:
        """åŠ è½½æŸ¥è¯¢æ¨¡å¼"""
        return {
            'select_all': {
                'patterns': [
                    r'æ˜¾ç¤º.*?æ‰€æœ‰',
                    r'æŸ¥çœ‹.*?å…¨éƒ¨',
                    r'åˆ—å‡º.*?æ‰€æœ‰',
                    r'show.*?all',
                    r'select.*?all'
                ],
                'confidence': 0.9
            },
            'count': {
                'patterns': [
                    r'ç»Ÿè®¡.*?æ•°é‡',
                    r'æœ‰å¤šå°‘',
                    r'è®¡ç®—.*?æ€»æ•°',
                    r'count'
                ],
                'confidence': 0.85
            },
            'aggregate': {
                'patterns': [
                    r'å¹³å‡.*?å€¼',
                    r'æœ€å¤§.*?å€¼',
                    r'æœ€å°.*?å€¼',
                    r'æ€»å’Œ',
                    r'avg|max|min|sum'
                ],
                'confidence': 0.8
            }
        }


    def _build_entity_mapping(self) -> Dict:
        """æ„å»ºå®ä½“æ˜ å°„"""
        return {
            'table_aliases': {
                'å‘˜å·¥': 'employees',
                'é›‡å‘˜': 'employees',
                'éƒ¨é—¨': 'departments',
                'ç”¨æˆ·': 'users'
            },
            'column_aliases': {
                'å§“å': 'name',
                'åå­—': 'name',
                'è–ªèµ„': 'salary',
                'å·¥èµ„': 'salary',
                'éƒ¨é—¨': 'department'
            }
        }